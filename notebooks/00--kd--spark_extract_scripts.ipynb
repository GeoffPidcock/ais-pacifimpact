{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "### Setup spark session"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import geomesa_pyspark\nconf = geomesa_pyspark.configure(\n    jars=['/usr/lib/spark/jars/geomesa-hbase-spark-runtime_2.11-2.1.0-m.2.jar'],\n    packages=['geomesa_pyspark','pytz'],\n    spark_home='/usr/lib/spark/').\\\n    setAppName('MyTestApp')\n\nconf.get('spark.master')\n# u'yarn'\n\nfrom pyspark.sql import SparkSession\n\nspark = ( SparkSession\n    .builder\n    .config(conf=conf)\n    .getOrCreate()\n)", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>50</td><td>application_1599131715985_0051</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-250-1-28.ec2.internal:20888/proxy/application_1599131715985_0051/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-250-1-62.ec2.internal:8042/node/containerlogs/container_1599131715985_0051_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Import dependencies and create datastore connection "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "params = {\n\"hbase.zookeepers\": \"hbase.optix-ons-local:2181\",\n\"hbase.catalog\": \"ons-historical\"\n}\n#ee refers to exactearth specifically. Other datasets we have include: orbcomm and adsbx\nfeature = \"ee\"\nee = ( spark\n.read\n.format(\"geomesa\")\n.options(**params)\n.option(\"geomesa.feature\", feature)\n.load()\n)\nee.createOrReplaceTempView(\"ee\")", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Running the query"}, {"metadata": {}, "cell_type": "markdown", "source": "### Lautoka"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Make sure the position column is used for the bounding boxes. query will not work with lat/long\n#position is a boolean and pyspark breaks down when trying to export a boolean to csv, hence the st_asText wrapper around position\n#allows us to export the data into csv\n#The st_makeBBOX is a bounding box wrapper, a point to remember: 'X' is LONGITUDE and 'Y' is LATITUDE when working with geographic data.\n#The timestamp is in days, the code below is extracting the data for the last 100 days for the defined boundary box.\n#The current code takes ~10 seconds to run, so we can make either the boundary box larger, or the timestamp larger\ndf = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(177.2597,-17.7725,177.637,-17.4265), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Coalesce saves file as one csv output. Else the data is exported as multiple csvs\ndf.coalesce(1).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/Suva\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Suva"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Running same query for Suva\ndf = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(178.0403,-18.4516,178.8282,-17.8369), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Extracting the data"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(1).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/Suva\")", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "An error was encountered:\nInvalid status code '400' from http://ip-10-250-1-28.ec2.internal:8998/sessions/33/statements/8 with error payload: \"requirement failed: Session isn't active.\"\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Cook Islands"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(-159.796901,-21.21187,-159.77495,-21.190184), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(4).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/cook-islands\")", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Solomon Islands"}, {"metadata": {}, "cell_type": "markdown", "source": "Honiara"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(159.813175,-9.499527,160.137272,-9.237509), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 3, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(4).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/honiara-solomon-islands\")", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Noro"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(157.131933,-8.302952,157.260289,-8.180972), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(4).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/noro-solomon-islands\")", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Vanuatu"}, {"metadata": {}, "cell_type": "markdown", "source": "Port Vila"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(168.141321,-17.824701,168.39538,-17.648119), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#increasing no. of csvs to make the output run faster. Prevents significant lag in saving\ndf.coalesce(10).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/port-vila-vanuatu\")", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Luganville"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(167.143315,-15.544086,167.244407,-15.498857), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(15).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/luganville-vanuatu\")", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Kiribati"}, {"metadata": {}, "cell_type": "markdown", "source": "Port of Betio"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(172.899429,1.330346,172.965518,1.404222), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(5).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/betio-kiribati\")", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Palau"}, {"metadata": {}, "cell_type": "markdown", "source": "Port of Malakal"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df = spark.sql(\"\"\"\nSELECT\ndtg, mmsi, vessel_name, callsign, vessel_type_code, vessel_class, length, width, flag_country, flag_code, longitude, latitude, st_asText(position), vessel_type, vessel_type_cargo, vessel_type_main, vessel_type_sub,\ndestination, eta, draught, sog, cog, rot, heading, nav_status, nav_status_code, source, message_type\nFROM\nee\nWHERE st_contains(st_makeBBOX(134.44567,7.321713,134.467089,7.340101), position)\nAND dtg < date_add(current_timestamp(), -1)\nAND dtg > date_add(current_timestamp(), -365)\n\"\"\")", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.coalesce(15).write.option(\"header\", \"true\").csv(\"s3://optix.ons.jupyter/jupyter/kdhingra/malakal-palau\")", "execution_count": 5, "outputs": []}], "metadata": {"kernelspec": {"name": "pyspark3kernel", "display_name": "PySpark3", "language": ""}, "language_info": {"name": "pyspark3", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "pygments_lexer": "python3"}}, "nbformat": 4, "nbformat_minor": 2}